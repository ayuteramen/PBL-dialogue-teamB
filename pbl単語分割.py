# -*- coding: utf-8 -*-
"""PBL単語分割.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FCDvEqO8sE4adYT3TW0kDizuKKxoFL8n
"""

ans1 = []
ans2 = []
with open('/content/tweet_pairs_removed.txt', 'r') as f:
    for line in f:
        data = []
        data = line.split("\t")
        if  data[1] != '' or data[2] != '\n':
            ans1.append(data[0] + data[1])
            ans2.append(data[2])

with open('col1.txt', 'w') as f1:
    with open('col2.txt', 'w') as f2:
        for i in range(len(ans1)):
            f1.write("%s\n" % ans1[i])
            f2.write("%s" % ans2[i])

! pip install janome

from janome.tokenizer import Tokenizer
t = Tokenizer("/content/dict.csv", udic_type="simpledic", udic_enc="utf8")

fin1=open('col1.txt')
col1=fin1.read()
fin1.close()

fin2=open('col2.txt')
col2=fin2.read()
fin2.close()

with open('tag_train_src.txt', 'w') as f3:
    for token in t.tokenize(col1):
        f3.write(token.surface)
        if token.surface != "\n":
            f3.write(" ")

with open('tag_train_tgt.txt', 'w') as f4:
    for token in t.tokenize(col2):
        f4.write(token.surface)
        if token.surface != "\n":
            f4.write(" ")